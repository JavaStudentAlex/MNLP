{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language modeling with simple MLP\n",
    "\n",
    "- Manual backprop\n",
    "- Improve training loop\n",
    "\n",
    "Sources: \n",
    "- https://github.com/karpathy/nn-zero-to-hero\n",
    "- https://github.com/karpathy/makemore\n",
    "- https://huggingface.co/course/chapter6/6?fw=pt\n",
    "\n",
    "Resources:\n",
    "- https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue_no_trainer.py\n",
    "- https://huggingface.co/course/chapter1/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the names.txt file from github\n",
    "!wget -O input.txt https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  for w in words:\n",
    "\n",
    "    #print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr[0]\n",
    "Ytr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init HP\n",
    "vocab_size = len(itos)\n",
    "emb_dim = 2\n",
    "block_size = 3\n",
    "h_dim = 100\n",
    "lr=0.1\n",
    "max_steps=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random batch\n",
    "g = torch.Generator().manual_seed(42) # for reproducibility\n",
    "C = torch.randn((vocab_size, emb_dim), generator=g)\n",
    "W1 = torch.randn((emb_dim * block_size, h_dim), generator=g)\n",
    "b1 = torch.randn(h_dim, generator=g)\n",
    "W2 = torch.randn((h_dim, vocab_size), generator=g)\n",
    "b2 = torch.randn(vocab_size , generator=g)\n",
    "batch_size= 32\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunkated forward pass\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(-1, 6)\n",
    "hpreact = embcat @ W1 + b1\n",
    "h = torch.tanh(hpreact)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logits, h, hpreact, embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recap chain rule: h(x) = f(g(x)) ==> h'(x) = f'(g(x))g'(x) | df/dx = df/dg * dg/dx\n",
    "# Derivative of loss for single observation\n",
    "\n",
    "# Loss\n",
    "\n",
    "# Layer 2\n",
    "\n",
    "# Tanh\n",
    "\n",
    "# Layer 1\n",
    "\n",
    "# Concatenation\n",
    "\n",
    "\n",
    "# Embedding\n",
    "\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "g = torch.Generator().manual_seed(42) # for reproducibility\n",
    "C = torch.randn((vocab_size, emb_dim), generator=g)\n",
    "W1 = torch.randn((emb_dim * block_size, h_dim), generator=g)\n",
    "b1 = torch.randn(h_dim, generator=g)\n",
    "W2 = torch.randn((h_dim, vocab_size), generator=g)\n",
    "b2 = torch.randn(vocab_size , generator=g)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "batch_size = 32\n",
    "losses = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "# with torch.no_grad():\n",
    "step = 0\n",
    "for i in range(max_steps):\n",
    "  # Batching\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "  \n",
    "  # Forward pass\n",
    "  # Embedding\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer 1\n",
    "  hpreact = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  # Linear layer 2\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  # Loss\n",
    "  loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "\n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward() # use this for correctness comparisons, delete it later!\n",
    "  \n",
    "  grads = []\n",
    "  # # manual backprop\n",
    "  # # -----------------\n",
    "  # # -----------------\n",
    "\n",
    "  \n",
    "  # update\n",
    "  for p, grad in zip(parameters, grads):\n",
    "    p.data += -lr * p.grad # (using PyTorch grad from .backward())\n",
    "    # p.data += -lr * grad\n",
    "\n",
    "  # track stats\n",
    "  losses.append(loss)\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  losses.append(loss.item())\n",
    "\n",
    "  if i > 10000:\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop from last session\n",
    "lr=0.1\n",
    "max_steps = 100000\n",
    "losses = []\n",
    "batch_size = 32\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # Batching ==> Replace with batches from pytorch dataloader\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,)) \n",
    "    \n",
    "    # Forward pass ==> Replace with call to our pytorch model\n",
    "    emb = C[Xtr[ix]] # (32, 3, 10)\n",
    "    h = torch.tanh(emb.view(-1, block_size*emb_dim) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    \n",
    "    # Reset gradients ==> Replace with the functions of our pytorch model\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "        \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Stochastic gradient descent ==> Replace with an optimizer from pytorch\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model from last session\n",
    "vocab_size = len(itos)\n",
    "emb_dim = 2\n",
    "block_size = 3\n",
    "h_dim = 200\n",
    "g = torch.Generator().manual_seed(42) # for reproducibility\n",
    "C = torch.randn((vocab_size, emb_dim), generator=g)\n",
    "W1 = torch.randn((emb_dim * block_size, h_dim), generator=g)\n",
    "b1 = torch.randn(h_dim, generator=g)\n",
    "W2 = torch.randn((h_dim, vocab_size), generator=g)\n",
    "b2 = torch.randn(vocab_size , generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "# Forward pass from last session\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "# Forward pass\n",
    "emb = C[Xtr[ix]] # (32, 3, 10)\n",
    "h = torch.tanh(emb.view(-1, block_size*emb_dim) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ytr[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim=2, block_size=3, hidden_dim=100, vocab_size=27, *args, **kwargs) -> None:\n",
    "        # Define components and hyperparamters of your model\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.block_size = block_size\n",
    "        self.cat_dim = embedding_dim * block_size\n",
    "        self.C = torch.nn.Embedding(vocab_size, embedding_dim) # Why do we use torch.nn.Embedding?\n",
    "        self.dense = torch.nn.Linear(embedding_dim*block_size, hidden_dim)\n",
    "        self.out = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        # Define how a forward pass is carried out\n",
    "        emb = self.C(x)\n",
    "        h = self.dense(emb.view(-1, self.cat_dim))\n",
    "        h = F.tanh(h)\n",
    "        logits = self.out(h)\n",
    "\n",
    "        if y is not None:\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        return logits, loss if y is not None else logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of weights in Pytorch?\n",
    "model(Xb,Yb)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected loss without training\n",
    "-torch.tensor(1/27).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "@torch.inference_mode() # @torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for batch in loader:\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        _, loss = model(*batch)\n",
    "        # Logging our metrics\n",
    "        losses.append(loss)\n",
    "    mean_loss = torch.tensor(losses).mean().item()\n",
    "    model.train() # reset model back to training mode\n",
    "    return mean_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader | Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement dataset\n",
    "class NameDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, Y=None) -> None:\n",
    "        # Setup the data\n",
    "        super().__init__()\n",
    "        # This could include loading and preprocessing of the data\n",
    "        self.X = X\n",
    "        if Y is not None:\n",
    "            self.Y = Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Get the lenght of the dataset\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get an instance of the dataset given and index\n",
    "        return self.X[idx], self.Y[idx] if self.Y is not None else self.X[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting everything togehter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.01\n",
    "max_epochs = 10\n",
    "batch_size=32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Dealing with GPU and CPU training\n",
    "\n",
    "losses = []\n",
    "\n",
    "train_dataset = NameDataset(Xtr, Ytr) # Creating our own train dataset\n",
    "val_dataset = NameDataset(Xdev, Ydev) # Creating our own validation dataset\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # Wrap the train dataset into a dataloader\n",
    "val_dataloder = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False) # Wrap the validation dataset into a dataloader\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # Define an optimizer\n",
    "\n",
    "print(f\"Number of training instances: {len(train_dataset)}\")\n",
    "print(f\"Number of training batches per epoch: {len(train_dataloader)}\")\n",
    "\n",
    "max_steps =  max_epochs * len(train_dataloader) # Compute the maxium number of steps for logging purposes\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "step = 0\n",
    "for epoch in range(max_epochs):\n",
    "    val_losses = []\n",
    "\n",
    "    # Set model in training mode\n",
    "    model.train()\n",
    "    # ix = torch.randint(0, Xtr.shape[0], (batch_size,)) \n",
    "    for batch in train_dataloader:\n",
    "        # Move to device\n",
    "        batch = [t.to(device) for t in batch] # Why do we iterate through the elements of batch?\n",
    "        # Forward pass\n",
    "        # emb = C[Xtr[ix]] # (32, 3, 10)\n",
    "        # h = torch.tanh(emb.view(-1, block_size*emb_dim) @ W1 + b1) # (32, 200)\n",
    "        # logits = h @ W2 + b2 # (32, 27)\n",
    "        # loss = F.cross_entropy(logits, Ytr[ix])\n",
    "        logits, loss = model(*batch) # What does *batch do?\n",
    "        # Reset gradients\n",
    "        # for p in model.parameters():\n",
    "        #     p.grad = None\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Stochastic gradient descent\n",
    "        # for p in parameters:\n",
    "        #     p.data += -lr * p.grad\n",
    "        optimizer.step()\n",
    "\n",
    "        # track stats\n",
    "        losses.append(loss)\n",
    "        \n",
    "        step += 1\n",
    "\n",
    "    # Logging (customize to your needs)\n",
    "    print(f'Loss: {step:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    \n",
    "    # Evaluate after each epoch (customize to your needs)\n",
    "    eval_loss = evaluate(model, val_dataloder, device)\n",
    "    print(f'Val loss: {step:7d}/{max_steps:7d}: {eval_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(losses).view(-1, len(train_dataloader)).mean(1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Exercises:\n",
    "- Train a model on the CBOW architecture using our dataset\n",
    "- Experiment with different optimizers, learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pen & Paper Exercises:\n",
    "- Do the math and show that our gradient for the cross entropy loss is correct\n",
    "- Do the math and show that our gradient for the linear layer is correct\n",
    "- Given the following model: $f(x) = w_3x^3+w_2x^2+w_1x+w_0$; optimize the models parameters (i.e., $w_0,w_1,w_2,w_3$) using stochastic gradient descent, given the following training examples (x,y): {(1,3),(-1,-5),(0,-3)} and the squared error loss $(y - f(x))^2$. Update the models parameters after each example. Use a learning rate $\\eta = 0.1$ and initialize all parameters to $1$ (i.e., $w_0=w_1=w_2=w_3=1$).\\\n",
    "Solution:\\\n",
    "(1,3): $w_0=w_1=w_2=w_3=0.8$\\\n",
    "(-1,-5): $w_0=-0.2$, $w_1=1.8$, $w_2=-0.2$, $w_3=1.8$\\\n",
    "(0,-3): $w_0=-0.76$, $w_1=1.8$, $w_2=-0.2$, $w_3=1.8$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "72768256ce09aebce0ac4dd5209fef3a694ce4cdd1a70ecd6912bedba09dd60c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
